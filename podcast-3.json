{"podcast_details": {"podcast_title": "Software Engineering Daily", "episode_title": "Cross-functional Incident Management with Ashley Sawatsky and Niall Murphy", "episode_image": "http://softwaredaily.wpengine.com/wp-content/uploads/powerpress/SED_square_solid_bg.png", "episode_transcript": " Incident management is the process of managing and resolving unexpected disruptions or issues in software systems, especially those that are customer-facing or critical to business operations. Implementing a robust incident management system is often a key challenge in technical environments. Rootly is a platform to handle incident management directly from Slack and is used by hundreds of leading companies including Canva, Grammarly, and Cisco. Ashley Sawotsky leads developer relations at Rootly and previously led Shopify's incident communications team. Nile Murphy is co-founder and CEO at Stanza. He has written extensively about reliability engineering and is the co-author of the best-selling book, Site Reliability Engineering. Ashley and Nile join us in this episode to discuss how engineers and their non-technical counterparts can successfully approach incident management together. This episode of Software Engineering Daily is hosted by Jeff Hemin. Check the show notes for more information on Jeff's work and where to find him. Ashley Sawotsky, Nile Murphy, welcome to Software Engineering Daily. Thank you. Thanks, Jeff. Imagine that you're building a new feature for your product. You write the code, prepare the PR, and ship it. A day later, you notice some weird movement in core product metrics. Was it because of the new feature or something else? Everyone wants to make data-driven decisions, but that's easier said than done. Data-driven decisions often means expensive point solutions and a ton of data work without conclusive results. StatSig was built to give engineering teams a single data tool to build better products. StatSig is unique because it combines modern feature flags with automated A-B testing and experimentation. This means that engineers can automatically measure the impact of every feature they ship on all product metrics, no data work required. Scaled startups like Versel, Notion, Brex, Whatnot, and Vanta use StatSig to ship features faster and safer. StatSig has a super developer-friendly free tier and simple usage-based pricing for enterprises. Whether you're getting started on your data journey or are just tired of overpaying for flag analytics and A-B testing tools, StatSig can help. Visit statzig.com slash softwaredaily to learn more. StatSig is offering software daily listeners access to a special startup package plus white glove onboarding support. So today's topic is incident management. Ashley, you work for a company called Rootly that works in the incident management field. What exactly is it that you do? And then we'll talk about that in a second. Yeah, so I am Rootly's developer relations advocate slash incident response advocate, reliability advocate, all things incident response and reliability. I work a little bit on the external side in the community, creating content and information centered around how organizations can level up their incident response, especially tech companies. And prior to joining Rootly, I spent about seven years at Shopify building out a lot of the incident management and communications processes there. So taking all that knowledge into a really cool product that actually automates and streamlines a lot of the processes that take place during an incident. Cool. And Nile, do you want to quickly introduce yourself? Sure thing. My name is Nile Murphy. I'm a CDO of a startup in the SRE space called Stanza Systems, which has just recently gone into kind of private beta. I am extremely uncomfortable with being called technical as well as being a CEO. But nonetheless, this is apparently what people see me as. So here we are. If your listeners know my name, it's probably because of the SRE book, the Site Reliability Engineering book, which was quite popular and still continues to be sold to this day. Fantastic. So what is your relation to Rootly, if any, for our listeners' understanding? Yeah, I'm just a hanger on really. I am interested in the incident management space. I should say that Stanza does not compete with Rootly or Jelly or Shoreline or anything else in the incident management space. We're doing something pretty different. But I do kind of pay a lot of attention to the incident management space generally because I think it's one of the intellectually most interesting areas in computer science in production as it's practiced today, because it's full of exceptional behavior and things you wouldn't expect. It's a very interesting space to be in. I like that you used the word intellectual rather than technical. I mean, you've made it clear that you're not a fan of the word as applied to yourself. But I'm reading into this that incident management is more than just a technical thing, but also includes wider aspects, such as informing customers and so on. Is that fair? Did you mean to imply that? Well, I say intellectually addressing precisely because of the fact that it touches on so many verticals of which technical resolution, understanding of distributed systems, etc. Obviously a hugely important component, but there are incredibly important other components as well. And actually Ashley is more qualified to talk about those. So we should talk to her about that. Sure. I think something that I often say when people ask what one of the most important skills you can have as an incident responder is, I think it goes on both whether you're a technical responder or in the communications or any other side is just the ability to reap the room. Incidents vary a lot and there's a lot of emotions and all sorts of things that are kind of going on in the background in addition to the actual technical response and the action items that are taking place. And I think that those are some of the most undervalued or skills that we don't call out as much as maybe we should. Just the ability to read the room and kind of respond accordingly and anticipate what's needed from you from that perspective. In addition to the actual remediation actions you're taking when something breaks. Speaking of reading the room, I think the traditional approach to incident management is the classic war room where you physically get everyone into a room. Now, of course, in the last couple of years, that approach has drastically shifted with obviously COVID and then everyone still working from home. How important do you think the physical presence of physical co-location of incident responders still is or would be? Maybe Niall has a different perspective to me. I would say physical co-location is not important at all during an incident. I've responded to probably hundreds of incidents and aside from really major events like at Shopify, for example, Black Friday. Sure, we're all in the room together. But generally speaking, a lot of companies run on Slack these days or just remotely in general. And I think I don't think it's necessary to be physically in the same room at all. Yeah, I think I mean, let's start off with the fact that I live in a tiny rain-soaked island on the other side of the world and have partaken in incidents in such a context for many decades. So I don't view the physical location as being necessary, although there is some kind of optimization effect for kind of small groups of people in physical co-location, I think. But whether or not the size of that effect is determinant with respect to success in the incident management piece, as opposed to being able to access expertise all over the world. Like, I don't know that the size of that effect would dominate every other concern. In fact, I think it probably doesn't. I was just going to say that I think there is a distinction between asynchronous and synchronous communication. And sometimes synchronous communication definitely does need to take place during an incident. But, you know, there's so many tools like Zoom and Google Meets and all of these different things that allow for even Slack huddles are really great for ad hoc synchronous communication. So that has just become a lot easier to do without sharing a physical space. I think for those of us in the reliability space as well, one of the other considerations that I used to hear a lot, but I don't quite hear so much anymore. I think there's a very valid reason for it. But one of the concerns I used to hear is, OK, you're the team that's on call for Zoom. Are you going to use Zoom the product to debug Zoom the product? Well, the answer is probably not, because if you do, when Zoom is down, you reach for your Zoom channel in order to coordinate. And actually, it's down. So the point that was typically made is your coordination mechanism for incident resolution should be distinct from the product that you are supporting, which I think most people would would get behind. But even in Google, there was still a very strong instinct to crack open a Google Doc for coordination. By default, we should say even for those some of the folks supporting Google Docs, because it's quite rare for the whole thing to go down. Like often you can get some bit or some shard or whatever to be functioning. Anyway, no, that's a really good point. And I have to admit, I was wondering about that myself. If we had time towards the end of this, I was going to ask it. But let's spoil it now. Then, Ashley, at Rootly, do you use Rootly to manage incidents about Rootly? Of course we do. When possible. You know, of course, if something is going wrong with Rootly, the platform, we luckily have a very small, very agile engineering team that is really responsive. And we are at that stage where, you know, we are a startup. We're all kind of on call all the time in some way. And if something breaks, we're on top of it. We as a company run on Slack and Rootly also integrates with Slack. So, of course, if we're experiencing like Slack's having an outage, then we have fallbacks that we can use to make sure we can still communicate if that takes place. But luckily, Slack is a wonderfully reliable platform that very rarely does that to us. Yeah, Slack is hands down my favorite, what do we call it, work instant messaging tool. You only really see the value of Slack once it's taken away from you. And I think the worst I had to work with at some point was, I don't know, I don't want to, I don't want to slag off any competitors in case there'll be future guests on a podcast. But Slack is definitely up text. And Rootly specifically works through Slack pretty much exclusively. Is that right? Yeah, we do have a web platform as well where you can see, you know, things like metrics and the history of your incidents. But when you're actively managing an incident, most people do so using our integration within Slack. I think Slack is, well, if the company has Slack, that will already be used for coordination during incidents and not during incidents anyway. So to me, that's a really natural fit. I'm thinking of a time in my first job when I was working for a startup, and it was my first job. I was working for a really hot startup and it went into my head, I think is very fair to say, in hindsight. I thought I was the shit. And we were managing some sort of customer facing incident and I was fixing it, you know, it was the old days, literally SSHing into production servers and restarting services and changing configuration files manually and what have you. And one of our more recent employees, she was working in marketing basically and PR and that, Hi Lauren, came by like physically to my desk in the office and asked me what was going on and what she could tell the customers. And I was really hugely dismissive. And I said, I'm busy fixing this, this is way more important than, you know, what you do. We'll talk about it afterwards. So I think that is a really kind of key story as it relates to the title of this podcast, bridging the gap between the technical and the non-technical responders and whatever Nile classifies himself as. It did during instance. Do you have any kind of, which we call it, war stories or anecdotes about similar things happening? And how have you generally gone about improving that kind of aspect of a company's culture? Yeah, I can speak a little bit to that to start. I came upon incident response in a pretty unusual way, I think. Like I have fully a communications background before I was at Shopify. I was at Disney during guest communications, not a technical person. I had no idea. I probably couldn't have even named like three programming languages. I started working at Shopify. I had no idea how any of the under the hood was working. And as we were building out our incident response program, started more in the customer facing space and eventually grew to work more closely with engineering. So I had a huge learning curve when it came to what was going on in incidents that were, you know, like a platform outage or something of a technical nature. And I think at the beginning, it was a lot of Googling to be honest. I didn't want to bother our engineers. You know, you were in that position where you had some comms person in your ear going, what does this mean? What am I supposed to say? That's super annoying. And I didn't want to be that person. But I also had to admit to myself that I had no idea what was going on. So fortunately, Google is your friend. I probably learned a whole lot about engineering and infrastructure just from Googling while following along in war rooms. And what I would do is just do my best to piece that together. And I would just write something based on my understanding and give it to our engineers. I luckily had some close ones that were really helpful in helping me understand what was going on. Shout out to people like Ryan McEmoil, who's a Glass Genius now, who is one of those people at Shopify who was very patient with me and would help me adjust comms. But I think part of it is just not being so afraid of that technical jargon. I think a lot of people see, like especially comms people, we see what's going on and we are like, I don't know, this is so over my head. I don't know. But if you just start where you are, start at the company you're in, learn the key pieces of the tech stack that tend to be involved in incidents. Learn what Kubernetes is at a very high level, like whatever it is, just start small term by term, Google by Google. You don't need a computer science degree. Just kind of piece it together. I think it's like a good place to start. It's not as scary as it looks. Yeah, I think just responding to a few things that Ashley said there, one of the lines they used to say at Google is, how do you solve a problem inside Google if you can't Google it like that was, which I thought was a good line, right? Like, oh my God, I can't use the tool that everyone else uses for problem resolution. It's a bit reminiscent of the scene in the IT crowd where the IT manager convinces the boardroom that if you type Google into Google, the internet will crash. Exactly. So from that point of view, I suppose, it's surprising the extent to which I don't know what X means. I will just look it up and see if somebody has written some kind of reasonable explanation for it. Like, well, that actually takes you an incredibly far distance when it comes to the mechanics of restoring service. That's obviously a very different act, creating a service from nothing. Or we should say the fundamentally creative act of writing software. But there is a kind of mechanical difference to restoring service and problem resolution and so on. When you are already confronted by a structure which has some flaw or some problem in it, or maybe the problem is with your model of it, which we'll come back to in a moment, but there's something in front of you which has some detail and it's like, find the problem. And it's a little bit like, I don't know if you play chess or if anyone listening to this plays chess, but there's a you occasionally get problems like white to play in mate in two or something. And it turns out there's a big intellectual difference between you are told there is a thing here for you to find and the task is finding it through a big tree of possibilities or cloud of uncertainty or however you would put that, versus playing on when you've no idea like what is what the situation on the board actually represents. So there's a difference between those two activities kind of cognitively. And the other piece, I'd say just attempting, however, inaccurately to channel the technical persona for a moment. The technical persona in the context of instant response often has to wrestle with this question of what's the model of the situation versus what is happening? Like in some sense, an incident is definitionally a situation where the model has varied from what the reality is doing. And that's always kind of very interesting, right? Because you have a model that the producer consumer system you are looking after works in the following way. And then it turns out it doesn't or it does. But there's some difference between what you thought would happen and what is actually happening. And anyway, the technical persona is often, if not primarily, wrestling with questions of cognitive models and knowledge about the world and questions of rapidity of change as well, because your model kind of been accurate at Tuesday at 16.15 and then inaccurate at Tuesday at 16.17. And those are always kind of interesting moments. Between the kids being home and hosting, everything in our house gets used up in summer. With Instacart, I can save money by stocking up on all my favorite summer brands. I save time by getting everything delivered in as fast as an hour. And I save myself a sink full of dirty dishes by stocking up on paper plates for the annual summer cookout. Save more on summer essentials? Spend more time enjoying summer. Add summer to CART. Download the app to get free delivery on your first three orders. Offer valid for a limited time. Minimum $10 per order. Additional terms apply. Want to see how users actually experience your website or app? Full Story helps brands understand their users' digital experiences to optimize what's successful and eliminate friction. Full Story's award-winning platform gathers extensive data on each user experience in real time, giving dev teams hundreds of ways to better understand both issues and successes in aggregate. Plus, with individual user sessions, Full Story enables you to drill down into the details, giving you a better understanding of each unique experience happening on your website or app. Visit fullstory.com to learn more. I suppose the challenge in communication comes partly from the fact that these models are different for different personas. If you're a systems engineer, you'll have one model which is relatively close to an actual, let's say, UML diagram or architectural model. But then if you are an end customer, you have a completely different model from what reality should look like. And then if you are a developer, there's yet another perspective. And then if you work in communications, there's yet another perspective. So how do we navigate that complexity, which I see is particularly difficult because there are other complexities. For example, the architecture itself is a complexity, but there are literally people hired to understand and manage that complexity. But here the complexity stems from the fact that there are different roles and different perspectives, and there isn't one person, unless I'm mistaken, whose job it literally is to understand the complexity that is different models. So how do we go about that? I think one thing as a communicator that stands out is your job as a communicator is really to understand your audience. And so having a bit of a lack of understanding it sometimes of the technical complexity going on can actually be a bit of a superpower. Because if we had our engineers write all of our customer-facing comms, they would be in-depth and deeply accurate and completely confusing. And unless you are communicating with a deeply technical audience, it doesn't really matter how insightful and intelligent and accurate those comms are, because they have no idea what you're talking about anyway. So I think as a communicator, you can use that filter to say, do I understand this well enough to explain it in a way that is meaningful and relevant to our audience? And you can use that to navigate where you might need to dig a little deeper and get more of a technical understanding to present things accurately, but also where maybe you need to pare back and find ways to simplify. I like to pay attention to when big companies are having incidents, how they communicate. And you can always tell when an engineer is writing the status page messaging versus when they've had somebody from more of a communications or customer side filter through that and explain the difference between what's happening and the impact of what's happening to customers. And I think if there's one thing communicators need to zoom in on, it's that, it's the impact of the problem and not worrying about having this in-depth understanding of the problem itself and what that resolution is going to be. That's definitely part of it. I think also, just coming back to your remark earlier, Geoff, about whose job it is to model the thing or what happens on the level when there's a conflict of models or the model resolution area of attention and so on is variable. So this is a pretty interesting question because, like, arguably, I wouldn't say this is achieved, right, but arguably one of the potential benefits of using SRE approaches to things is you're trying to understand the system holistically, right? You are trying to integrate in some sense, multiple models. I remember, I think this is somebody else's line. I know I'd be very grateful if it was mine, but I don't think it is. I went to a conference and on one slide or in one talk, there's a picture of lots of machines, you know, they all have lines to each other. And we hand wave away the fact that they're connected by network. And then the next talk, there's a picture of a Cisco and a Juniper. And then there's lines out to machines which don't have different names. So there's a cloud saying server rack or whatever. And that's one another kind of model. And then, you know, a third talk and there'd be an architecture diagram or organisational diagram of the people and their reporting lines and so on. So, like, yes, different models exist, but one of the most interesting questions around software, I think, is what can you successfully ignore and still do your job or do the task or whatever? And so what we find when we deal with highly complex models or highly complex systems is that we're building and discarding models. As it turns out, we could no longer successfully ignore the fact that when we post this thing to this other thing, it doesn't work like 1.01% of the time. We have to find the reason for that. Or in other words, we have a model which when we stand back and look at it is kind of squishy and cloudy and vague and so on. And it has increasing hardness and specificity as we dive into the area in question and look at the actuality of how it works. And that particular bit of the system is crystallised when we're investigating the thing that we're looking at. And then perhaps it turns out that's not a contributing factor to what's going on. And so we zoom back out of that and then go elsewhere in the problem space in order to crystallise that model. So it's not really a question of competing models. It's a question of the resolution of your understanding at any particular moment. That's really interesting. It strikes me that there is a parallel between what you've just talked about, i.e. the complexity of navigating communications during incidents and the actual incidents. Because in both cases we have, let's call them, levels of abstraction. And at some point they don't work as expected and we need to figure out where the issue is. I find your line really interesting about what can we successfully ignore. Also, I think you talked about before you said, I forget your exact phrasing, but basically it was about when we build systems, there is something that we just rely on that we don't have to understand completely. So for example, if, just a really simple example, I put some sort of app on the Linux server, I don't need to understand all of the Linux kernel, for example. But if something breaks or more likely doesn't behave as expected, doesn't behave the way we modelled it, that gives rise to an issue. Do you guys have any thoughts about when and if to involve vendors? I know currently I'm working for a big corporate and basically any system needs to have enterprise vendor support in case that model is broken or doesn't behave the way we thought it would, that we are able to call in the vendor. Is that a consideration, I suppose especially as a start-up, you can't always afford to do that. Is that a consideration that you make after incidents or before incidents? How do you navigate that whole part of complex systems design? So you must understand that as the CEO of a vendor, I am here saying you must use vendors hugely for all your stuff. But actually, of course, there's the classic trade-off between build and buy and so on and so forth. And many organisations at different stages of their life cycle can actually go back and forth between different ways of doing this. So I wouldn't really say there's a one size fits all answer, but I will say that in instant management as an area generally, I think even with the proliferation of folks who are kind of writing tools in the space or selling tools in the space, I should say, it's still a huge wild west area. Like loads of stuff is done on internal belt buckles and chewing gum scripts and ad hoc docs and so on and so forth. I don't really look at what the industry is doing on average and go, yes, this is totally fine. And all of the products are just gilding the lily. Like, no, the average level or median level or whatever of how we do this is just not very sophisticated. Yeah, we also exist in the vendor space. So again, a little bit of bias there and we see plenty of build versus buy. I've decided between build versus buy before I worked at an incident response automation platform for this exact use case. And I would say like, of course we do get many customers approaching us or folks we've approached who have some sort of homegrown version of even an incident response tool and slack automation. A, we often see that people just underestimate how much can be done in this space and they think like, oh, it's just a little chat ops. Like we can build that no problem. But with what we offer, we go much, much deeper than that. So I often find that people who do lean towards build, what they're really looking for is flexibility and customization and they want the thing exactly how they want it to be. So with really we're in a good place with that because we are such a configurable platform that we actually tend to play very well with people who have already built some sort of homegrown incident response tool. Because we're able to configure really to do whatever they want to do, whereas some other vendors are going to be very opinionated. So I think you have to decide if you want something to come in that's very opinionated that's going to tell you exactly how to do the thing you want to do. Or if you want more flexibility and you know how you want to do it and you maybe need help with implementation. And then there's all sorts of other trade offs in terms of like, who's going to maintain this thing that you built, or is it going to be run off the side of the desk forever, will there be a long term plan to maintain it? What happens is it breaks then you know how much of your time is going to need to be devoted to that. So like that I said I'm sure we could do a whole episode on build versus buy and trade offs that come with that. One more question on the build versus buy though, it's a good example of what we talked about before that. If you dig deeper there is more complexity. Even if you buy, there could still be the nuance of is it self hosted, is it 100% hosted for you. So what is the case of Rootly? Is that all hosted for you? Is there a self host option for customers who are a bit more, I don't want to use the word paranoid, that's a bit cynical, but who have more tight regulations I'd say. That's a great question. I don't believe we have a self host option, we run on the cloud. I don't really believe that, I'm sure there are exceptions, I won't make a sweeping generalization but in this day and age, I don't think any SaaS product really exists completely independently. And a lot of the times if it does, I don't think you're actually putting yourself in a much better position from a reliability standpoint. Depending on your needs, it's probably a little naive to think that you can do some of these things like hosting better than these giant, extremely reliable cloud hosting services that exist out there. So I get the temptation for people to say like we don't want to ever rely on third parties, but I just don't know how realistic that actually is. And even if you do all your own hosting, again if you dig down a layer, you realize that you're still relying on the electricity company that provides their own data center with electricity or the person who supplies you water because otherwise your engineers can't be in the data center or whatever it is. There is always another level. Another question I had to get a bit back on track on the topic of reaching that gap. How do you decide, you've alluded to it before, how do you decide how much to tell your customers and in what technical detail? I remember as a young and arrogant engineer being a bit frustrated at times because we were having an incident, but we were telling customers that we were doing scheduled maintenance and I think it was to keep up the illusion of having more uptime or being more reliable. Is that probably why, in your opinion? That's really interesting. I think my opinion on you know how much to share with your customers. Again, probably no one size fits all answer different companies have different appetites for how transparent they choose to be about their systems publicly. Sometimes you're in agreement with some of those vendors and maybe you can't even mention them by name, based on your agreement, but I would say, my overall advice would be definitely don't do that. Don't lie. I wouldn't ever misrepresent or just outright lie about the circumstances. I think sometimes there are things that will be omitted. I typically, the things like I would tend to advise to just omit from communications aren't from the standpoint of trying to hide things, but more from just like, is this useful information to our customers? Is this actionable for them? Does it leave them with more questions than answers? I think sometimes there is a temptation to like, tell them everything. And it's in the spirit of we want to be transparent. We want them to know what's going on. But when you're a customer and you're impacted by an incident and this, whatever it is, piece of software you rely on isn't working, probably the last thing you want is to be inundated with a massive amount of technical details. When you're just like, okay, but what do I do? What does this mean for me? So I would say to put that at the forefront of your thinking is what do your customers actually need to know? If your customers are deeply technical, they probably need more technical information. If your customers are not technical at all, they probably just want you to tell them what to do or what not to do or what you can say. And also just being honest about what you know and what you don't. I think there's a lot of danger in speculating around like, it'll be fixed soon. Maybe when you maybe don't even know what is actually causing the problem yet. So just being careful around speculating and making those assumptions. But centering it all on like, what is actually valuable and actionable information for the people you're talking to is probably the best way to go. Yeah, I have some opinions here. All right. I mean, it's interesting because fundamentally this kind of communication is a communication implying every kind of attribute of that between humans and between different audiences. Like Ashley, you were saying earlier, you communicate technically if your audience is tech, primarily technical for this. But actually, of course, for a sufficiently large product or popular product or however you would put that, there's different audiences, right? And so part of the problem is, well, I could stick all of this data in an appendix, but some people would be annoyed by it or some people would skip straight to the index or whatever. Like, is there some media and useful media we can head for? And I do not think that there is a finite algorithm which leads us to the correct decision for this in every circumstance. I will say that my experience in the industry suggests that primarily the pressure to defang, detail, like remove the detail from stuff comes a lot from legal departments actually who believe that the more detail you supply, the more opportunity you give an antagonistically minded customer to assert in court that you behave poorly under law, XYZ or however you would put that. And so there's an, I think the way I would describe it in the industry overall, there is more of a tendency to remove stuff from communications than there is to put stuff in. More of a tendency from the business layer, from the technical layer, of course, it's the other way around. Because the technical folks supplying that detail are like, of course everyone wants to know host 1234 is down, like customers ABC through XQZ are affected by this in some notional sense. And so they would want to know that. And of course, not all of the time do they want to know all of that. And as Ashley says, centering around impact is often a better way to structure this. But also, again, just being realistic about it, impact is not always easy to quantify. And you also don't always know that impact at the moment when you are compelled to communicate. And so, for example, in the recent kind of data dog outage, there's essentially a long gap in communication because they're spending a lot of time trying to figure out what the root cause slash contributing factor is. They don't really have that much more to say other than, you know, are engineers considered 73 of the 146 possible options and have discarded 73 of them will get back to you soon. And that's very satisfying for an engineer because at least you can go OK, the progress bar is going across the screen, but it's much less satisfying for somebody outside of that culture because they're going, you considered 73 things. Oh my God, you wasted so much time. Like you should have considered one thing and the one thing should have been right, but you can't explain to them. Actually, there's no way that you can pick the right thing or like if you do, it's hugely statistically unlikely you would do that, etc, etc, etc. So like this is partially why I say it's such an interesting area because there aren't many algorithms which are going to result in determined success given all inputs. I will come back to this point about the statement you made about your company in your early career there, Jeff. Ultimately, it's a question of trust, trust management, right? So by going, oh yes, this is totally scheduled outage. It just happens to have been scheduled for 2.30 in the afternoon, our peak time or whatever it is, right? You can, of course, get away with that, right? And I'm sure loads of people do get away with it a few times. And then it'll come out somehow like somebody will brag to somebody else and some channel or other. It'll get copied around. And then what you have is, oh, I'm your customer and you lied to me because you thought it would be convenient. Well, it is exceptionally convenient for me to retain my dollars. Thank you very much. And so the company is putting at risk a sustained relationship with their customers on foot the convenience of a short term optimization, which most people would say is, shall we say, short term optimizing at best and is customer disrespectful at worst. Yeah, I think to put it into context a little in my view, it was mostly startup license. So as a startup, you can build technical debt in all sorts of ways. And I suppose if you don't have a dedicated communications team yet, then taking shortcuts there while you're working on making the whole process more robust is justifiable. I don't want to slack off. As a employer, they really are a fantastic company and actually one of the shining beacons of customer respect, if I can claim that in my unbiased opinion. But I really get your point about the bit you said about figuring out which solution to go for when solving a problem. And in hindsight, it's always really easy to know, which begs the question, why don't more engineers just use hindsight from the get go? Well, we like we haven't, we haven't fixed the time machine thing yet. So when we when we do ship 1.0 of that thing, we're like, all the time. Customers are going to love that once we figure that out, instant communications are about to get a lot easier. Very much so. But something on Niles point actually that just stood out to me, you brought up a few things around like often there's fear that is going to come from other stakeholders like your legal team or your PR team is like, oh, no, we have a product launch. We don't want to look bad in the press right now. How can we bury this or executives, you know, feeling that fear and saying, can we just lie? Like whatever it might be. I think a takeaway from that is if you can avoid having these conversations for the first time around your level of risk appetite and transparency, while an incident is actually happening when all the fear and reality of that situation is right in front of you. And try to have those conversations before you're in that position. And so your legal team can go into it because you know, a lot of their fear probably comes from the fact that if something does lead to litigation. Well, guess who's on the line for that. It's the legal team who looked at and approved the cons. So a lot of it comes down to fear and uncertainty around what is okay. And of course, people are going to lean to like the safest possible thing they can cling on to in an incident, which often is really sterile, generic comms that sound like a lawyer wrote them. So having like safe spaces to have those conversations and experiment with like, what would a risk look like here? What could go wrong before you're ever in that situation can alleviate a lot of that. I like how Ashley's effectively given us an example of anticipated hindsight here. So think about the problem before it occurs, it literally is the solution. Yeah, there are ways to do that. And incident response and SRE like they are really difficult because they are spaces where there's not a lot of safety to fail. The stakes tend to be high. So we have to sort of manufacture those with things like game days and tabletop exercises and simulations and you know, the ways that we can experiment a little in that space before you're on call and your, you know, palms are sweating and you're terrified that if something happens, you don't know what you're going to do. Awesome. Like just to say again, channeling the technical persona. One of the difficulties with the technical persona is that although you're obviously relying on comms folks, you're in a team kind of embedded with other people. There is something fundamentally alone about being on call and responsible for an incident, which is not very comfortable and which interacts poorly kind of cognitively, shall we say the emotions of being under threat, particularly in should I say at will employment environments mean that the true, shall we say creative sparks that you need to have when you are doing fundamentally difficult kind of instant response. Like these two things are really intention. And so when you're on your 14th hour of on call shift, which got extended because of a gigantic outage and no one really knows what's going on and it's late, you're tired, you haven't eaten and nonetheless, you're expect to do that complicated model building, de-validation, etc. While also having the threat of, you know, in some cultures, continued employment, etc. Like that's the fundamental sympathy of the root of the technical persona and their job in these kind of contexts. And I cannot resist squeezing one last plugin with that because you just teed me up so perfectly for a Nile. That cognitive overhead or whatever you want to call it is a lot of what we aim to solve for with Rootly in that, you know, we have a customer that's actually done a really good job of this, where they've set up an incident test that is actually like a little mock incident that really will prompt you through when any of their people start an on call shift for incident command. And you can just, you know, kind of shake the rust off and run this little test and it prompts you through and it says, hey, you're the incident commander. That means you're responsible for this, this and this. If you need, like, here's the rules you should be aware of during an incident. Here's a reminder that your status page hasn't been updated in 30 minutes. You should do that. And we just kind of take that away so that it doesn't have to all live in your head. And that's like one of the really great things about using automation is you don't have to use it for just your technical response. You can actually use it to create some psychological safety that you know that this tool is kind of helping you along. So yeah, that's that's something that I particularly love about some of the ways that we've seen our customers use automation. That sounds really cool. And I love the the idea of bringing not just having custom experience or developer experience, but here having on call support person experience built into your workflow. That's really cool. Remind us if people are interested in Rootly, where do they go to check it out? Rootly.com and we do do demos that are completely free and we can personalize them. So if you're interested in that, hit book a demo on our page and we'll set you up with someone who can walk you through the tool. Fantastic. As promised, we'll end on another one of my my shenanigans. My I think my first week on call, which was also just week free of me having a job like before that I was at uni. I still didn't know any of the words. I barely knew what engine X was and the terms production staging development were all new to me. So I mixed them up and someone flagged a very minor thing to me saying, oh, in our dev environment, this port is publicly exposed. Someone send me that like 9pm. No customer data. The company is barely known at all, so no one's going to specifically go in and attack it. And I figured better safe than sorry. No one's going to need dev anyway. So I decided that we should just shut the server down and deal with it the next morning. However, since I mixed up the words, I just put on the general Slack channel. We're shutting down production. Did you put an ad here on that? I'm sure that went over really well. Not at here at everyone. Of course, it's not just people who are who are there. Actually, thank you very much for coming on the show. I really enjoyed talking to you and looking forward to this episode being published. Thanks so much, Jeff. This was really fun. No worries."}, "podcast_summary": "In this podcast episode, Ashley Sawotsky from Rootly and Nile Murphy from Stanza discuss incident management and the collaboration between technical and non-technical teams. Incident management involves handling unexpected disruptions or issues in software systems that are critical to business operations. Both guests have extensive experience in incident response and offer valuable insights into effectively managing incidents.\n\nAshley Sawotsky works as a developer relations advocate at Rootly, a platform for incident management. She emphasizes the importance of understanding the audience when communicating incident updates. Tailoring the information to the audience's technical understanding and focusing on the impact of the problem can make the communication more effective. Ashley also highlights the value of being honest about what is known and what is not during an incident.\n\nNile Murphy, a co-founder and CEO at Stanza, discusses the intellectual complexity of incident management. He mentions that incident management requires understanding different models and resolving discrepancies between the model of the system and the actual situation. Nile also addresses the challenge of balancing technical detail with the need for clear and actionable communication during incidents. He suggests that legal considerations often drive the tendency to remove detail from communications, but stresses the importance of building trust with customers through transparent and honest communication.\n\nThe guests also touch on topics such as remote incident management, involving vendors in incident resolution, and the use of automation to streamline incident response processes. They agree that physical co-location is not essential in incident management, as remote collaboration tools like Slack can facilitate effective communication. They also acknowledge that the decision to involve vendors in incident management depends on various factors, such as the organization's risk appetite and the level of customization required.\n\nOverall, the discussion provides valuable insights into incident management, emphasizing the importance of clear and tailored communication, understanding different perspectives, and establishing trust with customers.", "podcast_guest": "Ashley Sawotsky", "podcast_highlights": "Main topics discussed in the podcast episode:\n1. Incident management and its importance in technical environments.\n2. Introduction to Rootly, a platform for incident management.\n3. The role of engineers and non-technical counterparts in incident management.\n4. The impact of COVID-19 on incident management and the shift to remote work.\n5. The importance of effective communication during incidents.\n6. The role of models in incident response and the challenges of different perspectives.\n7. The decision to build or buy incident management tools.\n8. Transparency and customer communication during incidents.\n\nNotable quotes and significant insights/revelations:\n- \"Incident management is the process of managing and resolving unexpected disruptions or issues in software systems, especially those that are customer-facing or critical to business operations.\" (0:16)\n- \"The ability to read the room is an important skill for incident responders.\" (9:08)\n- \"Physical co-location is not important during an incident. Synchronous communication can take place through tools like Slack and Zoom.\" (14:02)\n- \"Being able to understand and communicate the impact of an incident to customers is crucial.\" (21:14)\n- \"Transparency and honesty are important in customer communications during incidents.\" (30:24)\n- \"Decisions about incident response should be made before an incident occurs, considering the level of risk appetite and desired transparency.\" (39:57)\n\nSuggested chapter titles:\n1. Introduction to Incident Management and Rootly\n2. The Role of Engineers and Non-Technical Counterparts in Incident Management\n3. Communication Strategies During Incidents\n4. Models and Perspectives in Incident Response\n5. Build vs. Buy: Decision-making for Incident Management Tools\n6. Transparency and Customer Communication Best Practices"}